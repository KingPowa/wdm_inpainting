{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the custom Nifti Dataset\n",
    "import os\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from wdm.configuration.files import LMDBDatasetConfig\n",
    "from wdm.utils.transforms import Cropper, Resize3D\n",
    "from wdm.datasets.file_based import LMDBDataset\n",
    "from wdm.datasets.utils import instantiate_datasets\n",
    "from wdm.utils.masking import DirectorySampler\n",
    "from wdm.configuration.mask import PresampledMaskConfig\n",
    "from wdm.datasets.mri import MRIMaskedDataset\n",
    "from wdm.dataloaders.inpainting import MRIInpaintDataLoader\n",
    "from wdm.utils.logging_tools import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected datasets:\n",
      "-- IXI Dataset (Location: test_data/IXI/lmdb/volumes_T1w)\n",
      "-- IXI Dataset (Location: test_data/IXI/lmdb/volumes_T1w)\n",
      "-- IXI Dataset (Location: test_data/IXI/lmdb/volumes_T1w)\n"
     ]
    }
   ],
   "source": [
    "# Config load\n",
    "CONFIG_FILE = \"wdm_inpainting/train/wdm.yaml\"\n",
    "\n",
    "config = OmegaConf.load(CONFIG_FILE)\n",
    "\n",
    "datasets = instantiate_datasets(config.datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<wdm_inpainting.configuration.files.LMDBDatasetConfig at 0x16cf346b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_configuration = LMDBDatasetConfig(\n",
    "    lmdb_folder=\"/Users/kingpowa/Documents/Programming/test_data/IXI/lmdb\",\n",
    "    is_volume=True,\n",
    "    modality=\"T1w\",\n",
    "    mtransforms=[Cropper((192, 160))]\n",
    ")\n",
    "dataset_configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lmbd_dataset = LMDBDataset(dataset_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_sampler = DirectorySampler(PresampledMaskConfig(\"masks\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(167, 192, 160)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_shape = next(iter(lmbd_dataset))[0].shape\n",
    "img_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48, 48, 48)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_size = Resize3D.suggest_optimal_size(img_shape, 3.5)\n",
    "opt_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms as tf\n",
    "from temp_transforms import MaskTransform\n",
    "\n",
    "tfs = {\"img\": [Resize3D(opt_size, \"trilinear\")], \"mask\": [Resize3D(opt_size, \"trilinear\"), MaskTransform()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "          [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           ...,\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "           [0., 0., 0.,  ..., 0., 0., 0.]]]]),\n",
       " tensor([30.4175,  0.0000]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = MRIMaskedDataset(lmbd_dataset, age_range=None, mask_sampler=mask_sampler)\n",
    "next(iter(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mri_dataloader = MRIInpaintDataLoader(lmbd_dataset, age_range=None, mask_sampler=mask_sampler, batch_size=1, num_workers=0, transforms=tfs)\n",
    "mri_dataloader.setup()\n",
    "train_mri_dataloader = mri_dataloader.train_dataloader() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img : torch.Tensor\n",
    "mask : torch.Tensor\n",
    "cond : torch.Tensor\n",
    "img, mask, cond = next(iter(train_mri_dataloader))\n",
    "batch = (img, mask, cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 1, 48, 48, 48]),\n",
       " torch.Size([1, 1, 48, 48, 48]),\n",
       " torch.Size([1, 2]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, mask.shape, cond.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = tuple(b.to(device) for b in batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_image(img, slice=65):\n",
    "    # Visualize the mask\n",
    "    \n",
    "    plt.figure(figsize=(4, 4))\n",
    "    plt.imshow(img[:,slice,...].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(img[0, 0], slice = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(mask[0, 0], slice = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wdm.model.wdm import WDM\n",
    "from wdm.model.diffusion.diffproc import SpacedDiffusion\n",
    "from wdm.model.diffusion.sampler import create_named_schedule_sampler\n",
    "from wdm.model.unet import UNetModel\n",
    "from wdm.model.utils.etc import create_model_and_diffusion, model_and_diffusion_defaults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIFFUSION_CONFIG = config.diffusion_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = model_and_diffusion_defaults(config.diffusion_config)\n",
    "args[\"diffusion_steps\"] = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image_size': 64,\n",
       " 'num_channels': 128,\n",
       " 'num_res_blocks': 2,\n",
       " 'num_heads': 4,\n",
       " 'num_heads_upsample': -1,\n",
       " 'num_head_channels': -1,\n",
       " 'attention_resolutions': '16,8',\n",
       " 'channel_mult': '',\n",
       " 'dropout': 0.0,\n",
       " 'use_checkpoint': False,\n",
       " 'use_scale_shift_norm': False,\n",
       " 'resblock_updown': True,\n",
       " 'use_fp16': False,\n",
       " 'use_new_attention_order': False,\n",
       " 'dims': 3,\n",
       " 'num_groups': 32,\n",
       " 'in_channels': 8,\n",
       " 'out_channels': 0,\n",
       " 'bottleneck_attention': True,\n",
       " 'resample_2d': False,\n",
       " 'additive_skips': False,\n",
       " 'mode': 'default',\n",
       " 'predict_xstart': False,\n",
       " 'use_conditional_model': None,\n",
       " 'learn_sigma': False,\n",
       " 'diffusion_steps': 50,\n",
       " 'noise_schedule': 'linear',\n",
       " 'timestep_respacing': '',\n",
       " 'use_kl': False,\n",
       " 'rescale_timesteps': False,\n",
       " 'rescale_learned_sigmas': False}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, diffusion = create_model_and_diffusion(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = create_named_schedule_sampler(\"uniform\", diffusion,  maxt=args[\"diffusion_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./results\n",
      "Training requires CUDA.\n"
     ]
    }
   ],
   "source": [
    "wdm = WDM(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    batch_size=1,\n",
    "    in_channels=1,\n",
    "    microbatch=-1,\n",
    "    lr=1e-03,\n",
    "    log_interval=10,\n",
    "    img_log_interval=10,\n",
    "    schedule_sampler=sampler,\n",
    "    mode=\"Conditional_always_known_only_healthy\",\n",
    "    label_cond_weight=0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdamW (\n",
       "Parameter Group 0\n",
       "    amsgrad: False\n",
       "    betas: (0.9, 0.999)\n",
       "    capturable: False\n",
       "    differentiable: False\n",
       "    eps: 1e-08\n",
       "    foreach: None\n",
       "    fused: None\n",
       "    lr: 0.001\n",
       "    maximize: False\n",
       "    weight_decay: 0.0\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wdm.configure_optimizers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]]),\n",
       " tensor([[[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           ...,\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]],\n",
       " \n",
       "           [[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            ...,\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "            [0., 0., 0.,  ..., 0., 0., 0.]]]]]),\n",
       " tensor([[74.0260,  0.0000]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = Session(\"WDM\", config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/kingpowa/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "/Users/kingpowa/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/configuration_validator.py:68: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkingpowaalias\u001b[0m (\u001b[33mkingpowaalias-universitatsklinikumtubingen\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>wdm_training/WDM_exec_1737390504.020138/logs/wandb/run-20250120_172824-kqrhr1yn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project/runs/kqrhr1yn' target=\"_blank\">UNetModel#1737390504.020138</a></strong> to <a href='https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project' target=\"_blank\">https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project/runs/kqrhr1yn' target=\"_blank\">https://wandb.ai/kingpowaalias-universitatsklinikumtubingen/UNetModel_project/runs/kqrhr1yn</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kingpowa/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/kingpowa/Documents/Programming/wdm_training/WDM_exec_1737390504.020138 exists and is not empty.\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | UNetModel | 251 M  | train\n",
      "--------------------------------------------\n",
      "251 M     Trainable params\n",
      "0         Non-trainable params\n",
      "251 M     Total params\n",
      "1,005.257 Total estimated model params size (MB)\n",
      "524       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "/Users/kingpowa/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b60fdf9f49b8493ab1af1232db509858",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    570\u001b[0m     ckpt_path,\n\u001b[1;32m    571\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    572\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 981\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1025\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 205\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 140\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:252\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 252\u001b[0m             batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmanual_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/manual.py:94\u001b[0m, in \u001b[0;36m_ManualOptimization.run\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m suppress(\u001b[38;5;167;01mStopIteration\u001b[39;00m):  \u001b[38;5;66;03m# no loop to break at this level\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_restarting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/manual.py:114\u001b[0m, in \u001b[0;36m_ManualOptimization.advance\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;66;03m# manually capture logged metrics\u001b[39;00m\n\u001b[0;32m--> 114\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m kwargs  \u001b[38;5;66;03m# release the batch from memory\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 319\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:390\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Programming/wdm_inpainting/model/wdm.py:112\u001b[0m, in \u001b[0;36mWDM.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    110\u001b[0m t_load \u001b[38;5;241m=\u001b[39m t_fwd \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mt\n\u001b[0;32m--> 112\u001b[0m lossmse, sample, sample_idwt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_cond_dilated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m t_fwd \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mt_fwd\n",
      "File \u001b[0;32m~/Documents/Programming/wdm_inpainting/model/wdm.py:141\u001b[0m, in \u001b[0;36mWDM._step\u001b[0;34m(self, image, cond, label, label_cond, label_cond_dilated)\u001b[0m\n\u001b[1;32m    139\u001b[0m info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m()\n\u001b[0;32m--> 141\u001b[0m lossmse, sample, sample_idwt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel_cond_dilated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabel_cond_dilated\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;66;03m# compute norms\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Programming/wdm_inpainting/model/wdm.py:239\u001b[0m, in \u001b[0;36mWDM.forward_backward\u001b[0;34m(self, batch, cond, label, label_cond, label_cond_dilated)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 239\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_loss_dict(t, {k: v \u001b[38;5;241m*\u001b[39m weights \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mitems()})\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/torch/_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    580\u001b[0m     )\n\u001b[0;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/torch/autograd/graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/torch/autograd/function.py:292\u001b[0m, in \u001b[0;36mBackwardCFunction.apply\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03mThis class is used for internal autograd work. Do not use.\u001b[39;00m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m    293\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;03m    Apply method used when executing this Node during the backward\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 16\u001b[0m\n\u001b[1;32m      8\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m      9\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpu\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     11\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[ModelCheckpoint(dirpath\u001b[38;5;241m=\u001b[39msession\u001b[38;5;241m.\u001b[39mworking_directory, monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss/MSE\u001b[39m\u001b[38;5;124m\"\u001b[39m, save_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)],\n\u001b[1;32m     12\u001b[0m     logger\u001b[38;5;241m=\u001b[39mwand_logger\n\u001b[1;32m     13\u001b[0m )\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmri_dataloader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 538\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ai-env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from wdm.utils.logging_tools import AdvancedWandLogger\n",
    "\n",
    "wand_logger = AdvancedWandLogger(model, session)\n",
    "\n",
    "# Define the trainer\n",
    "trainer = Trainer(\n",
    "    max_steps=50000,\n",
    "    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "    callbacks=[ModelCheckpoint(dirpath=session.working_directory, monitor=\"loss/MSE\", save_top_k=1)],\n",
    "    logger=wand_logger\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(wdm, datamodule=mri_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
